# ESPN Article Data Pipeline

An end-to-end **ELT data pipeline** that ingests live ESPN API data, loads it into **Snowflake**, transforms it with **dbt**, and orchestrates everything using **Apache Airflow**.

I built this project for two main reasons:
1. **Sports + Data**: As a big sports fan, I discovered ESPNâ€™s public API and noticed that its daily articles endpoint wasnâ€™t widely used in other projects. The API returns JSON payloads, so I wanted to build a clean/queryable database of ESPN articles over time. Hopefully this will help make this public data more accessible to others.
2. **Learning Modern Data Tools**: I wanted hands-on experience with industry-standard tools like **Snowflake**, **dbt**, and **Apache Airflow**. This project gave me the opportunity to connect them all into a working pipeline.

---

## ğŸš€ Features
- **Automated ingestion** from ESPNâ€™s public API (Python requests â†’ JSON).
- **Snowflake landing zone** for raw `VARIANT` JSON.
- **dbt models** to normalize and transform JSON into fact/dimension tables.
- **Airflow DAG** for orchestration (extract â†’ load â†’ transform â†’ test).
- **Deduplication** to prevent repeated payloads.
- **Dockerized environment** for reproducibility.

---

## ğŸ“‚ Repository Structure
```text
.
â”œâ”€â”€ airflow/               # Airflow orchestration
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ news_pipeline.py
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ espn_dbt/              # dbt project
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/       # flatten JSON into structured tables
â”‚   â”‚   â””â”€â”€ analytics/     # fact/dimension tables
â”‚   â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ snapshots/
â”‚   â””â”€â”€ tests/
â”‚
â””â”€â”€ espn_etl/              # Python Extract/Load utilities
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ api_calls.py           # pulls data from ESPN API
    â”‚   â””â”€â”€ snowflake_connect.py   # inserts JSON into Snowflake
    â””â”€â”€ scripts/
        â””â”€â”€ load_videos.py         # CLI entrypoint for local runs

```
## ğŸ— Data Pipeline
```
        +-------------+
        |   ESPN API  |
        +------+------+
               |
               v
   +-----------+-----------+
   | Python Extract/Load   |   â† Request data from ESPN API + load to Snowflake
   +-----------+-----------+
               |
               v
   +-----------+-----------+
   |  Snowflake (RAW)      |   â† Store raw JSON (VARIANT column)
   +-----------+-----------+
               |
               v
   +-----------+-----------+
   | dbt (Transformations) |   â† Staging + analytics models
   +-----------+-----------+
               |
               v
   +-----------+-----------+
   | Airflow Orchestration |   â† DAG to run daily (ingest â†’ transform â†’ test)
   +-----------------------+
```

ğŸ—„ Data Model
The pipeline follows a layered approach:
RAW
- RAW_JSON.NEWS_RAW
- id (UUID)
- json_blob (VARIANT)
- sport, league
- created_at
  
STAGING
- STG_NEWS â€” flatten articles (IDs, headlines, timestamps, authors).
- STG_TEAMS â€” normalize team metadata.
  
CORE (Analytics)
- ARTICLES â€” deduplicated fact table of articles.
- TEAMS â€” team dimension.
ARTICLE_TEAM â€” bridge table (many-to-many link between articles and teams).
